{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of NBA Injuries from Performance Stats\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Bobby Baylon\n",
    "- Kyra Brandt\n",
    "- Jayson Gutierrez\n",
    "- Nathaniel Mackler\n",
    "- Stephen Rabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup:\n",
    "Please run the cell below before running any of the code cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SET UP (ALL IMPORT STATEMENTS GO HERE)\n",
    "\n",
    "#general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract [DOUBLE CHECK BEFORE SUBMISSION]\n",
    " Load management is a significant topic issue in the NBA as it involves the decision-making process of whether a player should play on game day in order to avoid injuries and ensure long-term health and success for both the player and their team. The goal of our project is to create a system for categorizing NBA players' injuries based on performance data. We use a dataset of player's performance statistics and injuries built from web-scraping the NBA API and injury data from Kaggle, which contains XX observations and 80 different features. After doing a combination of hand-design and multicollinearity feature selection, we explore differences in performance between K-Nearest Neighbors, SVM and Random Forest models using recall, f-beta and AUC/ROC curves as metrics. **We ultimately found THIS model using THESE features was the best. [ADJUST WHEN RESULTS]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "A hot topic within the current NBA world revolves around the idea of load management. The concept of load management dictates if a player should play on game day and, if they do, how many minutes. One hypothetical example is that an NBA organization might sit their star player out for a game halfway through the season because that player has been used at high rates in recent games. That organization might analyze their star's workload, ask the player how their body is responding, and assess that it's time to give their star a rest to prevent any risk of injuries, minor or major. \n",
    "Practicing load management seems like an obvious protocol for an organization in order to preserve both the player and the organization’s future health and success. A study conducted through the 2012-2015 seasons in 17 NBA teams demonstrated that significant factors in player injuries were high game loads and player fatigue <a name=\"lewis\"></a>[<sup>[1]</sup>](#lewisnote). This study reinforces that there’s an interaction between game load and injury risk, thus it's reasonable for fans, organizations, and players to assess minutes and games played to determine if a player should sit to prevent injury. Another study conducted using data from the 2017-2019 seasons found that other risk factors for injury were player age and position <a name=\"cohan\"></a>[<sup>[2]</sup>](#cohannote). This study indicates that seasoned players are more prone to injury and that player build’s are considerable factors in injuries due to the size expectations of certain positions. A relevant article highlighted a trend an increasing injury severity over 11 seasons, attributing the causes to things such as lengthening the season and an increase in athletic intensity<a name=\"kosik\"></a>[<sup>[3]</sup>](#kosiknote). These variables are valuable in selecting specific variables that should be used in a statistical analysis of injury likelihood from player performance.\n",
    "On the surface, it seems obvious to people outside of the NBA world that you would want to prevent major injuries for the players, that could even be career ending, and that forcing any individual to play regardless of their physical status would be unethical and harmful. However, traditional NBA fans and retired players argue that current players, especially star players, shouldn’t sit due to fatigue or minor injuries as it robs fans of their full experience at an NBA game. When the late great Kobe Bryant was asked why he disapproves of modern NBA players sitting out due to fatigue, he stated this reasoning perfectly, “Because you have a lot of people paying a lot of money to come see these athletes play, and they deserve to see that.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement [DOUBLE CHECK]\n",
    "\n",
    "The research problem is classifying NBA player injuries as non-injured, minor injury, or major injury based on their performance statistics, such as usage rating, minutes played, and games played, and demographics, like age, height, and weight. The purpose of this project is to develop an accurate model that can aid NBA organizations in their decision-making regarding load management and player preservation, as high game loads and player fatigue have been identified as significant factors in player injuries. By identifying common injury risk factors among NBA players based on position and player build, the model can inform preventative measures and training programs, promoting player health and success. As discussed in the background section above, the significance of this research lies in its potential to help NBA organizations improve player health and success, while enhancing the fan experience at NBA games by providing a high level of play while preventing major injuries for the players. In other words, predicting injuries will help players and coaches sustain longer, healthier careers and deliver the performance their fans deserve. This project's impact can contribute to the overall goal of promoting player health and success while also enhancing the fan experience at NBA games.\n",
    "\n",
    "In approaching feature selection, we use our basketball knowledge to determine which features would be intuitively important as well as analysis of the features to eliminate ones with high collinearity, since many performance statistics are linear combinations of others. \n",
    "\n",
    "In approaching model selection, we consider **KNN**, **SVM** and **random forest**. KNN is a strong algorithm for multi-class classification, which is importance since we aim to classify injury statuses in a hierachy of non-injured, minor injury or major injury. However, the high dimensionality of our initial dataframe presents a challenge in implementing KNN for this classification task, which further motivates good feature selection. We are not concerned about the high test time of KNN since, in industry, this algorithm would only have to be used around 10^4 or 10^5 times per year. \n",
    "\n",
    "SVM is a good fit for our project because it is an easily interpretable model. Interpretability is an important factor at play because if our model predicts that a player should sit out or take time off, players, coaches and fans are going to want to know why. Our model being interpretable is also important because, if accurate and interpretable, the model could provide motivation for adjustment of training regimes, leading to greater longevity in player careers. \n",
    "\n",
    "Finally, we explore random forest since random forest has built in feature selection. Thus, random forest provides both another model to consider for our project but also another route for feature selection for us to compare with what we come up with in our feature selection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We combined data from the Kaggle Injury Data 2010-2020 and the nba_api (both of which are described below) to create a final cleaned dataframe pairing individual player performance statistics with whether or not players were injured that particular season. If the player was injured, we also have a column for whether their injury was minor or severe. We will use this merged dataframe as the basis of our model.\n",
    "\n",
    "The dataframe was constructed from the datasets below, both of which are in the Repo (as is our final dataframe) if you wish to view them:\n",
    "\n",
    "**Injury Data 2010-2020 (Kaggle):**\n",
    "- Link: https://www.kaggle.com/datasets/ghopkins/nba-injuries-2010-2018?resource=download \n",
    "- Repo Link: https://github.com/COGS118A/Group014-Wi23/blob/main/injuries_2010-2020.csv\n",
    "- Dataset size: 27,106 X 5 = 27,106 observations and 5 variables\n",
    "- A single observation consists of the name of the injured player, the team they played for while injured, notes detailing the injury, injury leave, and/or return from injury, and the date for which the player either left on injury leave or returned to play. \n",
    "- Some critical variables are the Required, Relinquished, and Notes variables. Required and Relinquished (based on whether or not a name is present) indicate whether a player is going on injury leave or returning to the field of play. Both of these variables are represented as string values and are categorical. The Notes variable contains more specific information on the injury (i.e. did not play or day-to-day [which is questionable to play]) and/or indicates the beginning or end of injury leave for a player. The Notes variable is represented as a string. \n",
    "- **DATA CLEANING:** This dataset was cleaned using string parsing to determine if a particular injury was minor or severe. A minor injury was defined to be an injury that keeps a player out of the game for one game or less, while a severe injury was one that kept the player out for more than one game. We determined which type of injury was which using string parsing - for example, a string containing “dtd” (day-to-day) indicated a minor injury, while words like “ACL” indicated severe injuries. Due to the fact that one injury could be featured in the kaggle dataset anywhere from 1-10+ times, we whittled the dataset down to recording whether a player was uninjured, minorly injured, severely injured, or both in each season. *(To see this process more in depth, please see our DataCleaningEDA118a notebook, linked here: https://github.com/COGS118A/Group014-Wi23/blob/main/DataCleaningEDA118a.ipynb)*\n",
    "- **FINAL VERSION:** At the end of our cleaning, we had 3796 observations of 4 variables (see below): season_year, player name, and a boolean for minor and severe injury. This was left-joined with our performance stats from the NBI API dataset, and NaNs were replaced with falses, since not being in the injuries dataset means a player wasn’t injured. We did have to discard approximately 4% of the injury data due to names being stored in a different format; we expect that this will slightly bias our model to underpredict injuries, but the dataset was too large to realistically go through and fix this by hand given the time frame. *(As above, this process can be seen in depth in the DataCleaningEDA118a.ipynb notebook in our repo linked above.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clean_head](Clean_injury_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nba_api for web scraping (GitHub):**\n",
    "- Link: https://github.com/swar/nba_api\n",
    "- Repo Link: https://github.com/COGS118A/Group014-Wi23/blob/main/AdvStats09-23.csv\n",
    "- Dataset size: 6593 observations x 80 variables.\n",
    "- An observation consists of around 80 performance statistics for a given player (i.e. time played, points scored, number of rebounds, average free throws made, etc.) along with their name, player ID, some demographics (ex. age) and the season.\n",
    "- Critical variables are the player's name and season because these are important for matching our dataframes as well as the performance statistics and demographics since these are the features we will feed into our models. Of the many player statistics we have data for, we expect to focus on variables that highlight general player performance, like average time played, average points scored in a game, and average shots made which would all be represented as integer values. A core element of our project moving forward will be to do feature selection from these performance and demographic statistics. \n",
    "- **RETRIEVAL PROCESS:** We retrieved advanced statistics data for NBA players from the 2009-2010 season to the present season. We first queried the NBA website for a table that included the identification information of all players on record, which was then filtered to include only those who have played since the 2009-2010 season. This process of retrieval took advantage of the NBA_API's 'player.py' module which provides access to a static database of all players that the NBA has recorded statistics for. Then, for each season, advanced stats data is retrieved and filtered to only include players who played in that season using the NBA_API's leaguedashplayerstats endpoint that accesses the advanced statistics for NBA players under some specified criteria. The resulting dataframes of each season's advanced stats for each player are concatenated into a single dataframe containing advanced stats data for all NBA players who have played since the 2009-2010 season. The resulting dataframe can be used for further analysis of NBA player performance.\n",
    "\n",
    "![NBA_head](NBA_stat_head.png)\n",
    "\n",
    "**Final Dataframe**\n",
    "- Link: https://github.com/COGS118A/Group014-Wi23/blob/main/nba_api_merged_injuries\n",
    "- Dataset size: 6593 observations x 81 variables\n",
    "- A single observation consists of around 80 performance statistics connected for a single player for a single season and whether or not they sustained a severe injury or a minor injury. \n",
    "- A large portion of our project is the feature selection we did to determine what critical variables are. We used the variables for whether or not they were injured to as the ground truth labels for our model. \n",
    "- The data cleaning process used to create this dataframe is described above and more in depth in the DataCleaningEDA118a notebook (https://github.com/COGS118A/Group014-Wi23/blob/main/DataCleaningEDA118a.ipynb).\n",
    "\n",
    "![Merged_head](merged_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "**Feature Selection:** Of our 81 features in our final dataset, we need to perform feature selection to determine which of them are the most useful and which can be excluded. We make use of our intuition as a first step, eliminating obvious things like player_id, player_name, nickname and selecting one from redundant sets like team_id and team_abbreviation. Then we analyze the relationships between features since some performance statistics are linear combinations of others. Having eliminated those features, we can procede to model construction and selection. Additionally, the random forest model provides us with a sanity check of feature selection because this model has built in feature selection. \n",
    "\n",
    "**Model Selection:** As mentioned previously, K-nearest neighbors is an obvious solution for this problem. Realistically, one would need to predict the injury class of NBA players no more frequently than tens of thousands of times per year. Therefore, the high complexity of testing with KNN is not a huge issue. The curse of dimensionality is also not a significant problem, as we have thousands of data points but only tens of features. However, a large vulnerability would be the risk of useless features affecting our KNN classifier. This is why it is so important we do good feature selection and scale our data appropriately. SVM also presents another proposed solution. We experiment with an RBF kernel and look into others appropraite for a dataset with more observations than features as a potential hyperparameter. We also saw potential in exploring grid search CV as a means of doing further feature selection by using different values of C (the \"hardness of the margin\") and different sets of features (from the ones selected on intution and research as described above). Finally, random forests is a useful solution because of its interpretability and automated feature selection. This automated feature selection removes biases that our intuition may introduce and may reveal patterns we had not anticipated. \n",
    "\n",
    "Previous studies' models, such as that conducted by Lewis or Cohan & Schuster, have used only player age and minutes played, since intuitively, these two factors are by far the most important in predicting whether a player will be injured. For this reason, our benchmark model is the  regression using only age and minutes played for input, so if a complex model fails to do significantly better than this model, we can safely dismiss it as not useful. Additionally, in our feature selection, we will use those two features (age, minutes played) as our baseline for determining our additional performanace features are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "While there are certainly consequences for a false positive in this case (like potentially hurting a player's mindset or causing them to reduce their performance), the consequences of a false negative (an injury occuring that could have been prevented or whose probability could be reduced through better form/training moderation) are far worse. The obvious error metric for this case would be recall (TP/(TP+FN)), but this metric would give zero weight to false positives. The harm of false positives is not zero, just lower than that of false negatives. Therefore, I think the best metric would be an Fbeta metric. The value of beta is subjective. We will play around with a few values to determine the beta that “feels” right, but it should be over one to weight recall higher.\n",
    "\n",
    "We may also use confusion matrices and ROC/AUC in performing our model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results [NOT YET FINISHED]\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### 1: Feature Selection\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Preliminary KNN Model\n",
    "\n",
    "The full code for this can be viewed in: https://github.com/COGS118A/Group014-Wi23/blob/main/PreliminaryResults118a.ipynb\n",
    "\n",
    "We built two separate KNN models for our project. The first comprised our preliminary results section in the project checkpoints and due to time constraints was a binary classifier. This first model was implemented in a pipeline utilizing the default scaling settings. We fit this pipelin on the raining dataset and then used it to predict results for the testing dataset. Since everything was set to default and this was the most basic model we could build (especially given the time constraints), this preliminary KNN model serves as our \"default\" algorithm for characterizing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 0.6995412844036697, the recall is 0.7840616966580977, and the fbeta with beta=2 is 0.7655622489959839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x17f333b6ac0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PRELIMINARY MODEL:\n",
    "\n",
    "#read in data\n",
    "url = \"https://raw.githubusercontent.com/COGS118A/Group014-Wi23/main/nba_api_merged_injuries\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data.loc[:,[\"SEVERE_INJURY\", \"MINOR_INJURY\"]] = data[[\"MINOR_INJURY\", \"SEVERE_INJURY\"]].fillna(False) #replaces NAs with falses, because \n",
    "#NAs mean there was nothing in the injury set when we did the join\n",
    "\n",
    "features = data.columns[5:-3]\n",
    "X = data[features].copy()\n",
    "injuries = data.columns[-2:]\n",
    "y = data[injuries].copy()\n",
    "X = X.values\n",
    "# make this a two-class classifier with injured or not, treating minor and major as the same. This is just for preliminary results,\n",
    "# we will explore multi-class solutions later\n",
    "y['INJURY'] = (y['SEVERE_INJURY'] | y['MINOR_INJURY'])\n",
    "y = y[['INJURY']]\n",
    "y = np.ravel(y.values)\n",
    "\n",
    "#gotta do GroupShuffleSplit so we don't end up with people testing against themselves\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X=X, y=y, groups=data['PLAYER_NAME']))\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # Step 1: Preprocessing with StandardScaler\n",
    "    ('knn', KNeighborsClassifier()) # Step 2: KNN classification\n",
    "])\n",
    "\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "y_pred = knn_pipeline.predict(X_test)\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta):\n",
    "    precision, recall, fbeta, _ = precision_recall_fscore_support(y_true, y_pred, beta=beta, average=\"binary\")\n",
    "    return precision, recall, fbeta\n",
    "\n",
    "precision, recall, fbeta = fbeta_score(y_test, y_pred, 2)\n",
    "print(f\"The precision is {precision}, the recall is {recall}, and the fbeta with beta=2 is {fbeta}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrated the suitability of this intial model through precision, recall and F-beta metrics of the KNN algorithm for the binary classification. This preliminary exploration yielded a precision value of 0.69, suggesting this model identifies a greater number of true positives with some false positives. We got a recall value of 0.78 which implies that our initial model still results in 0.22 misclassifications (false negatives). While this score is larger than the precision found, we believe it is possible to perform even better. Lastly, our F-beta value was 0.76 with beta = 2 where recall is weighted twice as much as precision. F-beta scores are a measure of the model’s accuracy based on both the precision and recall. Getting a value of 0.76 means we are classifying more samples than not. Though these scores are not terrible, we have confidence in our abilities to improve our current model, or identify another one capable of yielding greater confidence in our predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: KNN Model with Selected Features\n",
    "\n",
    "The full code for this can be viewed in: https://github.com/COGS118A/Group014-Wi23/blob/main/KNN_Model_118A.ipynb\n",
    "\n",
    "Having thus established a baseline and completed our feature selection, we set to work constructing a second KNN model, this time using the 20 features we had selected. We used the same pipeline as we had for our baseline model and again used GroupShuffleSplit to avoid the same individual presenting in the training and testing set. We then performed grid search cross validation to determine the best value of k, checking every odd k from 1-72. We then extracted the k with the best f-beta score.\n",
    "\n",
    "As above, we then calculated the precision, recall and f-beta score for this model, which are printed below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = ['AGE', 'GP', 'W_PCT', 'MIN', 'E_OFF_RATING', 'E_DEF_RATING', 'AST_PCT',\n",
    "       'AST_TO', 'AST_RATIO', 'OREB_PCT', 'DREB_PCT', 'REB_PCT', 'E_TOV_PCT',\n",
    "       'USG_PCT', 'E_USG_PCT', 'PACE_PER40', 'PIE', 'POSS', 'FGA_PG',\n",
    "       'FG_PCT'] # importing the columns that we went through for manual feature selection\n",
    "\n",
    "X = data.loc[:,cols_list]\n",
    "y = data.iloc[:,-2:]\n",
    "y['INJURY'] = (y['SEVERE_INJURY'] | y['MINOR_INJURY'])\n",
    "y = y[['INJURY']]\n",
    "y = np.ravel(y.values)\n",
    "X = X.values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # as per TA feedback, we implement Group Shuffle Split to ensure each\n",
    "# player is either all in the train set or all in the test set, even if we have multiple years of data for them. \n",
    "train_idx, test_idx = next(gss.split(X=X, y=y, groups=data['PLAYER_NAME']))\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "def fbeta_score_gridsearchcv(y_true, y_pred, beta):\n",
    "    precision, recall, fbeta, _ = precision_recall_fscore_support(y_true, y_pred, beta=beta, average=\"binary\") #slight modification of error function for the gridsearch\n",
    "    return fbeta\n",
    "\n",
    "param_grid = {'knn__n_neighbors': range(1, 73, 2)}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_pipeline, param_grid=param_grid, scoring=make_scorer(fbeta_score_gridsearchcv), cv=5)\n",
    "\n",
    "##RAN INTO A PROBLEM\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_k = grid_search.best_params_['knn__n_neighbors']\n",
    "precision, recall, fbeta = fbeta_score(y_test, grid_search.predict(X_test), 2)\n",
    "print(f\"The best value of k is {best_k}.\")\n",
    "print(f\"The precision is {precision}, the recall is {recall}, and the fbeta with beta=2 is {fbeta}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, our model using the manually selected features resulted in a lower F-beta score than the original/preliminary model, which would seem to suggest a problem in our feature selection. Because with 80 features we have 2^80 possibly combinations of features, a grid search for feature selection would be too time and computationally intensive to feasibly complete. While we could try a random search, we decided it was a better choice to reevaluate once we had the feature selection from the Random Forest model to compare with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: SVM Model\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Random Forest \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion [NOT YET FINISHED]\n",
    "\n",
    "### Interpreting the result [NOT DONE]\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "- Heading into feature selection, we anticipated... but then we found... Knowing what we do now...\n",
    "- With respect to the KNN model, we found that using our hand selected features decreased performance from a model with all 80 features. We also found that the best value of k was XX. This suggests...\n",
    "- With respect to the SVM model, we settled on THIS kernel and THIS value of C, which produced THIS METRIC SCORE. This suggests...\n",
    "- Finally, with respect, to the Random Forest model, we found... The automatic feature selection of the Random Forest model emphasized A, B, and C in contrast with our hand selected features. This suggests...\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?\n",
    "\n",
    "While we were happy with the results we were able to achieve, cosntructing a model that could actually be used for the NBA would require a different form of data than the one we used. We predicted player injury from their stats for the whole year, but their whole year stats could have been directly affected by an injury early in the season. This introduces collinearity and confounds into the data that prevent it from being the best model for the problem. If we were to continue with this project, we would want to get better and more granular data. It would perhaps be better to look from game to game or to do some form of transformation to account for the way that injuries are affecting the data, but due to time constaints, we were not able to explore these possibilities for this project. \n",
    "\n",
    "Had we had more time, we think it would have been beneficial to explore the multiclass algorithm as we originally intended. Separating out minor and severe injuries may have made the classes more distinct and thus made it easier for the models to learn and perform better. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "We will be using data that is directly taken from the NBA.com website. We are aware that it is crucial to make sure the data is gathered methodically and objectively. We will be working with player statistics that are made available to the public via the website, and as the data will not contain sensitive information and is made available to the public, informed permission is not necessary. We will use prosportstransactions.com to access player injury data for the 2010–2011 season through the 2019–2020 season. We won't falsify the data to forward an objective, such as financial gain, so in order to account for honest portrayal and unintentional use of the data when doing our research.\n",
    "\n",
    "### Conclusion [NOT DONE]\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lewisnote\"></a>1.[^](#lewis): Lewis, M. (2018). It’s a Hard-Knock Life: Game Load, Fatigue, and Injury Risk in the National Basketball Association. J Athl Train. https://meridian.allenpress.com/jat/article/53/5/503/112788/It-s-a-Hard-Knock-Life-Game-Load-Fatigue-and<br> \n",
    "<a name=\"cohannote\"></a>2.[^](#cohan): Cohan, A., Schuster, J. Fernandez, J. (2021). A deep learning approach to injury forecasting in NBA basketball. Journal of Sports Analytics. <br>\n",
    "<a name=\"kosiknote\"></a>3.[^](#kosik) Kosik, K., Lundquist, K., & McInnis, K. (2021). Temporal Trends and Severity in Injury and Illness Incidence in the National Basketball Association Over 11 Seasons. Journal of Athletic Training, 56(1), 15-23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
